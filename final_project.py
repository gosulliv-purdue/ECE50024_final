# -*- coding: utf-8 -*-
"""final_project.ipynb

Automatically generated by Colaboratory.

"""

# !pip install tensorflow
# !pip install matplotlib
# !pip install numpy

"""Load the data set"""

# Base code taken from https://www.tensorflow.org/tutorials/keras/classification

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Download the datasets
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10,5))
plt.imshow(train_images[0])
plt.colormaps()
plt.show()

print(train_images.shape)
print(len(train_labels))
print(test_images.shape)
print(len(test_labels))

# Separate out a validation set from the test set
val_set_size = 5000
val_images, val_labels = test_images[:val_set_size], test_labels[:val_set_size]
test_images, test_labels = test_images[val_set_size:], test_labels[val_set_size:]


def noisy_labels(train_images,train_labels,num_flip=2000):
    if num_flip > len(train_images):
        num_flip = len(train_images)-1
    indicies_1 = np.random.choice(len(train_images),size=num_flip)
    indicies_2 = np.random.choice(len(train_images),size=num_flip)
    for i in range(len(indicies_1)):
        temp_img=train_images[indicies_1[i]]
        temp_label=train_labels[indicies_1[i]]
        train_images[indicies_1[i]] = train_images[indicies_2[i]]
        train_labels[indicies_1[i]] = train_labels[indicies_2[i]]
        train_images[indicies_2[i]] = temp_img
        train_labels[indicies_2[i]] = temp_label
    return train_images, train_labels

def class_imbalance(train_images,train_labels,classnum=0,class_less_num=3000):
    img_shape = tuple(map(lambda i, j: i - j, train_images.shape, (class_less_num,0,0)))
    label_shape = tuple(map(lambda i, j: i - j, train_labels.shape, (class_less_num,0,0)))
    print(img_shape)
    new_train_images = np.zeros(img_shape)
    new_train_labels = np.zeros(label_shape)
    k = 0
    num_class0 = 0
    for i in range(len(train_labels)):
        if train_labels[i]==classnum:
            num_class0 = num_class0 + 1
            # print(num_class0)
            if num_class0 > class_less_num:
                continue
        # print(train_images[i].shape)
        # print(new_train_images[k].shape)
        new_train_images[k] = train_images[i]
        new_train_labels[k] = train_labels[i]
        k = k+1
        # np.append(new_train_images,train_images[i])
        # np.append(new_train_labels,train_labels[i])
    return new_train_images, new_train_labels


# add training bias to the training data
train_images, train_labels = class_imbalance(train_images,train_labels)
train_images, train_labels = noisy_labels(train_images,train_labels)

train_images = train_images / 255.0
val_images = val_images / 255.0
test_images = test_images / 255.0



# Create all the models

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10)
])

model_baseline = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10)
])

model_val = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model_baseline.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model_val.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model_val.build(input_shape=(28, 28))


# Loss function that is used for the reweighting method
@tf.function
def loss_fn(y,z,sample_weight=None):
    if sample_weight is None:
        sample_weight=tf.ones_like(y)
    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y,z,sample_weight=sample_weight)

# Define the reweighting training function
def reweight_train(train_images,
                   train_labels,
                   val_images,
                   val_labels,
                   model,model_val,
                   num_epochs,
                   loss_fn,
                   test_images,
                   test_labels,
                   batch_size = 64,
                   val_batch_size = 64):
  
  len_dataset = len(train_images)
  len_val_dataset = len(val_images)
  tot_num_batch = int(len_dataset/batch_size)
  print('tot_num_batch', tot_num_batch)
  optimizer = tf.keras.optimizers.Adam()
  
  hist_tot = []
  for epoch in range(num_epochs):
      print("Start of epoch {}".format(epoch))
      # run through the entire training data per epoch
      for blah in range(tot_num_batch):
          # Create the training and validation batches
          batch_indices = np.random.choice(len_dataset, size=batch_size, replace=False)
          val_batch_indices = np.random.choice(len_val_dataset, size=val_batch_size, replace=False)
          x_batch_train = tf.cast(train_images[batch_indices],tf.float32)
          y_batch_train = tf.cast(train_labels[batch_indices],tf.float32)
          val_x_batch_train = tf.cast(val_images[val_batch_indices],tf.float32)
          val_y_batch_train = tf.cast(val_labels[val_batch_indices],tf.float32)

          with tf.GradientTape(persistent=True) as tape2:
              # tape2.watch(epsilon)
              # First Gradient run to update the parameters
              with tf.GradientTape(persistent=True) as tape:
                  epsilon = tf.Variable(tf.zeros((len(y_batch_train),),dtype=tf.float32))
                  # Need to make sure that the tapes are watching the important variable
                  tape2.watch(epsilon)
                  tape.watch(epsilon)
                  y_hat_logits = model(x_batch_train, training=True)

                  loss_f = loss_fn(y_batch_train, y_hat_logits,sample_weight = epsilon)
              # print("0",[var.name for var in tape.watched_variables()])
              # compute gradient to intermediately update model parameters
              grads = tape.gradient(loss_f, model.trainable_variables,unconnected_gradients=tf.UnconnectedGradients.ZERO)
              tape2.watch(grads)

              # grads = [tf.multiply(1.0, g) for g in grads]
  
              # update the parameters for the validation model
              tape2.watch(model_val.trainable_variables)
              for dim in range(len(model.layers)-1):
                # print(dim)
                # layer_weights = model_val.layers[dim+1].get_weights()
                # print(layer_weights[0].shape)
                model_val.trainable_variables[2*dim].assign(tf.subtract(model.trainable_variables[2*dim], grads[2*dim]))
                model_val.trainable_variables[2*dim+1].assign(tf.subtract(model.trainable_variables[2*dim+1], grads[2*dim+1]))
                # model_val.layers[dim+1].set_weights(layer_weights)


              # Apply the updated model to the validation data
              y_val_hat_logits = model_val(val_x_batch_train)
              loss_g = loss_fn(val_y_batch_train, y_val_hat_logits)

          #print("1",[var.name for var in tape2.watched_variables()])
          # Calculate the gradient of epsilon
          grads_epsilon = tape2.gradient(grads, epsilon,unconnected_gradients=tf.UnconnectedGradients.ZERO)
          # loss_grad_epsilon = tape2.gradient(loss_g, epsilon)

          weights = [max(-grads_epsilon[i],0.0) for i in range(len(grads_epsilon))]
          
          # Normalize the weights
          if sum(weights)>0:
              new_weights = weights/sum(weights)
          else:
              new_weights = weights
          new_weights = tf.squeeze(new_weights)

          
          with tf.GradientTape() as tape3:
            # Calculate the new loss function with the new weights
            y_hat_logits = model(x_batch_train, training=True)

            loss_f_hat = loss_fn(y_batch_train, y_hat_logits,sample_weight = new_weights)

          grads2 = tape3.gradient(loss_f_hat, model.trainable_variables,unconnected_gradients=tf.UnconnectedGradients.ZERO)
          optimizer.apply_gradients(zip(grads2, model.trainable_variables))
  
      # print('grads_epsilon',grads_epsilon)
      # print('grad_ofGrad_epsilon',grad_ofGrad_epsilon)

      hist_tot.append(model.evaluate(test_images,  test_labels, verbose=2))
  return hist_tot

batch_size = 64
val_batch_size = 64
num_epochs = 20
hist_out = reweight_train(train_images,train_labels,val_images,val_labels,
               model,model_val,num_epochs,loss_fn,test_images,test_labels)

# train the base model
baseline_hist = model_baseline.fit(train_images,train_labels, epochs=num_epochs,verbose=1)
